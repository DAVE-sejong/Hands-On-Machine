{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter11. 심층 신경망 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나의 문제점\n",
    "1. 토익공부하느라 공부할 시간이 너무 절대적으로 부족하다.\n",
    "2. 각 파트에 대해 깊이있게 설명하고 그림으로 만들기에 학습시간이 부족하다.\n",
    "3. 코드까지 덛붙이기에는 시간이 너무너무 부족하다.\n",
    "4. 12장은 진짜 몬소리인지 전혀모르겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장에서는 앞서 배웠던 신경망의 내용중 신경망의 오버피팅과 학습의 신뢰도와 성과의 개선방법을 파악할 것이다.\n",
    "앞서 activation fuction과 optimizer는 예를 들어 relu와 sigmoid, leaky relu 등 다양한 기법들이 가능하며, 각각의 특징들이 존재한다.\n",
    "신경망은 학습을 위해 무수히 많은 parameter와 layer를 사용하는 만큼, train의 overfitting에 취약하므로 \n",
    "학습이 적절히 되기 위해서는 batch normalization과 dropout, 초기화와 같은 방법을 통해, 학습의 성과를 개선하는 방법을 알아볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 신경망이 쉽게 가지는 문제로는\n",
    "1. vanishing gradient(반대로 exploding gradient) 문제가 있다. 이는 layer와 epoch와 관련이 되어있다. \n",
    "layer의 경우 늘어날수록 activation function을 여러번 지남으로 따라, optimizer과정 즉 weight의 미분과정에서 weight가 층이 깊어짐에따라\n",
    "0에 가까워 지거나 무수히 커지는 문제를 말한다.\n",
    "2. 학습의 목표는 높은 정확도를 가지는 것이고, 이를 위해 많은 parameter와 layer 또 epoch를 가져간다. 앞서 말한 요소들과 데이터가 커질 때,\n",
    "학습 속도의 저하가 일어나기 때문에 어떻게 빠른 속도로 높은 정확도를 얻는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanishing gradient를 먼저 알아볼 필요가 있다. layer가 무수히 많아질 경우 우리는 앞서 알던 backpropagation을 진행한다.\n",
    "이에 대하여 우리는 sigmoid를 activation funtion으로 설정할 경우, sigmoid 미분에 따라 sigmoid(y)*(1-sigmoid(y))를 진행하며 \n",
    "이 값은 0과 0.25사이 값을 가진다. \n",
    "layer가 늘어날 경우 미분에 따른 처음 layer는 무수히 많은 sigmoid 미분을 거쳐왔으므로 0~1사이의 값을 여러번 곱한 0에 가까운 값을 가진다.\n",
    "이에 따라, 학습이 잘 되어지지 않는 것을 vanishing gradient라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번쨰로 초기화이다. 초기화는 처음의 w와 b의 값을 어떻게 해놓는 것이 좋을까라는 아이디어이다. 이에 대해서는 레이어와 학습이 이후에도\n",
    "weight값들이 정규분포에 근접하는 것을 유도하는것을 의미한다. \n",
    "이에 대한 설명으로 https://gomguard.tistory.com/184 가 유용하다.\n",
    "간단히 설명하자면, 정규분포가 sigmoid와 같은 함수를 지나면 layer과 0과 1에 치우침으로, 앞서 보았던 vanishing gradient가 발생한다.\n",
    "그렇다면 분산을 1이 아닌 input양에 맞춰서 충분히 작게한다면 weight의 분포가 유지되어, 각 weight가 유의미해질수 있다는 의미이다.\n",
    "이에 대한 방법으로 대표적으로 xaiver와 he초기화가 존재한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 두번째로 activation function이다. 처음에는 sigmoid로 인한 vanishing gradient가 발생하였고, relu를 통해 이 문제를 해결할 수 있었다.\n",
    "하지만 이 밖에도 leakyrelu, elu, tanh 등이 존재한다. 이러한 다양한 이유는 backpropagation에서 어떤 방식이 weight update가 적절할지와\n",
    "관련되어 있다.\n",
    "sigmoid와 tanh은 유사하게 weight의 값을 줄여 vanishing gradient 문제를 가지고 있다. 또한 연산자체가 지수함수이므로 무겁다.\n",
    "relu(y)의 경우 y가 양수이면 미분값이 1일 경우, 역전파가 잘 이루어져서 앞선 vanishing gradient를 방지하며 빠를 수 있었다.\n",
    "하지만 y가 음수일 경우 weight가 update되지 않아, 죽은 relu라고 표현하기도 한다. 이에 대해서 leaky relu는 y가 음수일 경우,\n",
    "0.1이라는 미분값을 주어, 음수에서도 가중치 update가 일어나게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 세번째로 batch normalization이다. 이도 앞서 언급된 vanishing gradient와 exploding gradient를 방지하는 방법이다.\n",
    "이 방법의 아이디어는 layer의 값(각 층의 input)이 내부 공변랑 변화이다. 즉 이를 위해, 각 층의 input을 정규화해주는 것이다.\n",
    "이를 통해, 각 input의 평균과 분산이 일정하며 이에 따라, weight 변화량의 문제가 줄어들것이라는 아이디어라고 생각할 수 있다.\n",
    "하지만 각 알고리즘에 따른 방법의 차이가 있으며, 연산량 증가로 학습 시간의 증가를 포함한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네번쨰로 dropout이다. dropout은 각층의 뉴런(node or perceptron)중 일부만 랜덤하게 사용하는 것이다. 일반적으로 50%를 사용한다.\n",
    "이러한 간단한 방식은 실제로 규제와 ensemble과 같은 효과를 가진다고 보여지고 있다.\n",
    "1000개의 뉴런 중 매번 랜덤하게 500개만 사용할 경우, 학습이 overfitting 되어지지 않고 weight가 학습되므로 앞서 말한 vanishing gradient를\n",
    "방지할 수 있다. 또한 랜덤한 사용은 다양한 경우의 수를 만들어 냄으로 이는 ensemble과 비슷하다고 바라보기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 마지막으로 optimizer이다. optimizer는 대표적으로 gradient descent처럼 weight학습의\n",
    "방법이다. 우리는 여태까지, 빠르게 loss가 minimum이 되어지는 방식을 찾는 것이다. 이를\n",
    "위해서는 local minimum에 빠지지 않아야하며, 학습이 올바른 방향으로 나아가야하는 것이\n",
    "필요하다.\n",
    "이에 대한 방법으로, momentum, adagrad, rmsprop, adam optimizer 등이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. optimizer\n",
    "2. dropout\n",
    "3. activation function\n",
    "4. initializer\n",
    "5. batch normalization \n",
    "물론 본인은 능력이 없어서 강의를 들었던 강사의 코드를 가져온다.\n",
    "출처 :https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/cnn_mnist_basic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGES LOADED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "print (\"PACKAGES LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-e14e74943c0c>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\heon1\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\heon1\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\heon1\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\heon1\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\heon1\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/', one_hot=True)\n",
    "trainimg   = mnist.train.images\n",
    "trainlabel = mnist.train.labels\n",
    "testimg    = mnist.test.images\n",
    "testlabel  = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier Init\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "  \"\"\"Set the parameter initialization using the method described.\n",
    "  This method is designed to keep the scale of the gradients roughly the same\n",
    "  in all layers.\n",
    "  Xavier Glorot and Yoshua Bengio (2010):\n",
    "           Understanding the difficulty of training deep feedforward neural\n",
    "           networks. International conference on artificial intelligence and\n",
    "           statistics.\n",
    "  Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "  Returns:\n",
    "    An initializer.\n",
    "  \"\"\"\n",
    "  if uniform:\n",
    "    # 6 was used in the paper.\n",
    "    init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    return tf.random_uniform_initializer(-init_range, init_range)\n",
    "  else:\n",
    "    # 3 gives us approximately the same limits as above since this repicks\n",
    "    # values greater than 2 standard deviations from the mean.\n",
    "    stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "    return tf.truncated_normal_initializer(stddev=stddev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-0b613631b3a5>:22: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-0b613631b3a5>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\heon1\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 50\n",
    "batch_size      = 100\n",
    "display_step    = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_input    = 784 # MNIST data input (img shape: 28*28)\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 256 # 2nd layer num features\n",
    "n_hidden_3 = 256 # 3rd layer num features\n",
    "n_hidden_4 = 256 # 4th layer num features\n",
    "n_classes  = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "dropout_keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(_X, _weights, _biases, _keep_prob):\n",
    "    layer_1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])), _keep_prob)\n",
    "    layer_2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])), _keep_prob)\n",
    "    layer_3 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_2, _weights['h3']), _biases['b3'])), _keep_prob) \n",
    "    layer_4 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_3, _weights['h4']), _biases['b4'])), _keep_prob) \n",
    "    return (tf.matmul(layer_4, _weights['out']) + _biases['out']) # No need to use softmax??\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.get_variable(\"h1\", shape=[n_input, n_hidden_1],    initializer=xavier_init(n_input,n_hidden_1)),\n",
    "    'h2': tf.get_variable(\"h2\", shape=[n_hidden_1, n_hidden_2], initializer=xavier_init(n_hidden_1,n_hidden_2)),\n",
    "    'h3': tf.get_variable(\"h3\", shape=[n_hidden_2, n_hidden_3], initializer=xavier_init(n_hidden_2,n_hidden_3)),\n",
    "    'h4': tf.get_variable(\"h4\", shape=[n_hidden_3, n_hidden_4], initializer=xavier_init(n_hidden_3,n_hidden_4)),\n",
    "    'out': tf.get_variable(\"out\", shape=[n_hidden_4, n_classes], initializer=xavier_init(n_hidden_4,n_classes))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases, dropout_keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =pred, labels =y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.8).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Accuracy \n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "print (\"Network Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/050 cost: 0.420301497\n",
      "Training accuracy: 0.960\n",
      "Epoch: 001/050 cost: 0.131600570\n",
      "Training accuracy: 0.980\n",
      "Epoch: 002/050 cost: 0.093505665\n",
      "Training accuracy: 0.980\n",
      "Epoch: 003/050 cost: 0.074808923\n",
      "Training accuracy: 0.980\n",
      "Epoch: 004/050 cost: 0.061319740\n",
      "Training accuracy: 1.000\n",
      "Epoch: 005/050 cost: 0.053998114\n",
      "Training accuracy: 0.990\n",
      "Epoch: 006/050 cost: 0.044897743\n",
      "Training accuracy: 0.970\n",
      "Epoch: 007/050 cost: 0.039373854\n",
      "Training accuracy: 0.990\n",
      "Epoch: 008/050 cost: 0.034813633\n",
      "Training accuracy: 0.990\n",
      "Epoch: 009/050 cost: 0.032128718\n",
      "Training accuracy: 0.970\n",
      "Epoch: 010/050 cost: 0.028625195\n",
      "Training accuracy: 0.970\n",
      "Epoch: 011/050 cost: 0.025123352\n",
      "Training accuracy: 0.980\n",
      "Epoch: 012/050 cost: 0.022707582\n",
      "Training accuracy: 0.980\n",
      "Epoch: 013/050 cost: 0.020063873\n",
      "Training accuracy: 1.000\n",
      "Epoch: 014/050 cost: 0.019576946\n",
      "Training accuracy: 0.990\n",
      "Epoch: 015/050 cost: 0.016579549\n",
      "Training accuracy: 1.000\n",
      "Epoch: 016/050 cost: 0.017423877\n",
      "Training accuracy: 0.990\n",
      "Epoch: 017/050 cost: 0.015403967\n",
      "Training accuracy: 1.000\n",
      "Epoch: 018/050 cost: 0.013575084\n",
      "Training accuracy: 0.990\n",
      "Epoch: 019/050 cost: 0.013453328\n",
      "Training accuracy: 0.990\n",
      "Epoch: 020/050 cost: 0.011267135\n",
      "Training accuracy: 1.000\n",
      "Epoch: 021/050 cost: 0.011882217\n",
      "Training accuracy: 1.000\n",
      "Epoch: 022/050 cost: 0.010626153\n",
      "Training accuracy: 1.000\n",
      "Epoch: 023/050 cost: 0.010350091\n",
      "Training accuracy: 1.000\n",
      "Epoch: 024/050 cost: 0.009455808\n",
      "Training accuracy: 0.990\n",
      "Epoch: 025/050 cost: 0.010037516\n",
      "Training accuracy: 0.990\n",
      "Epoch: 026/050 cost: 0.008452553\n",
      "Training accuracy: 1.000\n",
      "Epoch: 027/050 cost: 0.008321641\n",
      "Training accuracy: 1.000\n",
      "Epoch: 028/050 cost: 0.008311267\n",
      "Training accuracy: 1.000\n",
      "Epoch: 029/050 cost: 0.007562950\n",
      "Training accuracy: 1.000\n",
      "Epoch: 030/050 cost: 0.007002412\n",
      "Training accuracy: 1.000\n",
      "Epoch: 031/050 cost: 0.007708110\n",
      "Training accuracy: 1.000\n",
      "Epoch: 032/050 cost: 0.006665254\n",
      "Training accuracy: 1.000\n",
      "Epoch: 033/050 cost: 0.006260284\n",
      "Training accuracy: 1.000\n",
      "Epoch: 034/050 cost: 0.006606544\n",
      "Training accuracy: 1.000\n",
      "Epoch: 035/050 cost: 0.005457499\n",
      "Training accuracy: 1.000\n",
      "Epoch: 036/050 cost: 0.005748783\n",
      "Training accuracy: 1.000\n",
      "Epoch: 037/050 cost: 0.005394075\n",
      "Training accuracy: 1.000\n",
      "Epoch: 038/050 cost: 0.004967235\n",
      "Training accuracy: 1.000\n",
      "Epoch: 039/050 cost: 0.004893983\n",
      "Training accuracy: 1.000\n",
      "Epoch: 040/050 cost: 0.005166714\n",
      "Training accuracy: 0.990\n",
      "Epoch: 041/050 cost: 0.004685545\n",
      "Training accuracy: 0.990\n",
      "Epoch: 042/050 cost: 0.004308804\n",
      "Training accuracy: 1.000\n",
      "Epoch: 043/050 cost: 0.005196924\n",
      "Training accuracy: 1.000\n",
      "Epoch: 044/050 cost: 0.004195835\n",
      "Training accuracy: 1.000\n",
      "Epoch: 045/050 cost: 0.004444983\n",
      "Training accuracy: 1.000\n",
      "Epoch: 046/050 cost: 0.004463828\n",
      "Training accuracy: 1.000\n",
      "Epoch: 047/050 cost: 0.004286988\n",
      "Training accuracy: 1.000\n",
      "Epoch: 048/050 cost: 0.004090158\n",
      "Training accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, dropout_keep_prob: 0.7})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, dropout_keep_prob:1.})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "        train_acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, dropout_keep_prob:1.}) \n",
    "        print (\"Training accuracy: %.3f\" % (train_acc))\n",
    "\n",
    "print (\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 부분적으로 보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. xaiver 적용\n",
    "\n",
    "기존 코드\n",
    "stddev = 0.05\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], stddev=stddev)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes], stddev=stddev))\n",
    "}\n",
    "\n",
    "xaiver 적용\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.get_variable(\"h1\", shape=[n_input, n_hidden_1],    initializer=xavier_init(n_input,n_hidden_1)),\n",
    "    'h2': tf.get_variable(\"h2\", shape=[n_hidden_1, n_hidden_2], initializer=xavier_init(n_hidden_1,n_hidden_2)),\n",
    "    'h3': tf.get_variable(\"h3\", shape=[n_hidden_2, n_hidden_3], initializer=xavier_init(n_hidden_2,n_hidden_3)),\n",
    "    'h4': tf.get_variable(\"h4\", shape=[n_hidden_3, n_hidden_4], initializer=xavier_init(n_hidden_3,n_hidden_4)),\n",
    "    'out': tf.get_variable(\"out\", shape=[n_hidden_4, n_classes], initializer=xavier_init(n_hidden_4,n_classes))\n",
    "}\n",
    "\n",
    "앞선 코드도 stddev를 0.05로 줄여주었지만 보통 random_normal을 쓰며, 이 경우 분산이 문제가 된다.\n",
    "그러므로 각 층마다 weight initiakuzer를 수정하여 적용할수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. dropout\n",
    "사용이 간단하지만 변화하는 점은 조금있다.\n",
    "\n",
    "i.dropout은 train의 경우 0.5이지만 test에서는 1이다. 그러므로, placeholder에 넣어주어야한다.\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "ii. 위와같이 각 층의 activation function후에 dropout을 층마다 추가해주어야한다.\n",
    "\n",
    "x_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, _weights['h3']), _biases['b3']))\n",
    "layer_3 = tf.nn.dropout(x_3, _keep_prob) \n",
    "\n",
    "iii. 앞서 placeholder와 같이 train의 학습과정에서는 dropout을 0.6으로 하지만\n",
    "cost및 test에서는 1.0으로 설정해주어야한다. \n",
    "\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feeds = {x: batch_xs, y: batch_ys, dropout_keep_prob: 0.6}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        feeds = {x: batch_xs, y: batch_ys, dropout_keep_prob: 1.0}\n",
    "        avg_cost += sess.run(cost, feed_dict=feeds)\n",
    "        print (\"TRAIN ACCURACY: %.3f\" % (train_acc))\n",
    "        feeds = {x: mnist.test.images, y: mnist.test.labels, dropout_keep_prob: 1.0}\n",
    "        test_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\"TEST ACCURACY: %.3f\" % (test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. activation function\n",
    "\n",
    "앞서 relu를 사용하였지만\n",
    "    x_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1']))\n",
    "    layer_1 = x_1\n",
    "    x_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2']))\n",
    "    layer_2 = x_2\n",
    "    \n",
    "에서 relu를 sigmoid, tanh, elu, leakyrelu 등으로 변경이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. optimizer\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) \n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "\n",
    "train.AdamOptimizer를 gradientdescent나 momentum등 자신이 원하는 optimizer로\n",
    "변환하면 간단하다. adam이 가장 범용성이 좋다. 물론 데이터의 특징마다 다른 optimizer가\n",
    "더 유용한 경우도 많으니 실험하는것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
